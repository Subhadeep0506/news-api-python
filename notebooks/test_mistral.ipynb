{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"0eHmmEGxp0208bpdMbOi0tYefvuU0beh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"open-mixtral-8x7b\",\n",
    "    temperature=0.2,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J\\'aime programmer.\\n(Note: This is a direct translation and assumes context where the subject \"I\" is understood. French sentences often require the use of the subject, even when it\\'s clear from context in English.)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence. Only give the translation, nothing else.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio2action\n",
    " \n",
    " \n",
    "Hello Everyone, Good Morning. Hope you all are having a great day. Today we are glad to present our paper entitled\n",
    "\"Audio2Action: A Framework for Automated Task and Insight Generation from Speech\" at the prestigious ICBAI conference. Before starting, I would like to thank my fellow authors for their contribution and support and specially thank Genpact India for their continued support and contributions to make this research project a successful one.\n",
    " \n",
    "The agenda of this presentation is to talk about the business challenge and motivation behind the study. The key goals or objectives of our research and methodology, followed by the solution design, dataset, evaluations and results. Finally we'll like to talk about the future scopes and what further improvements can be made to make this solution more sophisticated towards boosting productivity and efficiency with generative AI and speech AI.\n",
    " \n",
    "In today's fast paced business setting, it is extremely crucial to stay updated and on track via upskilling and collaborating with various teams and businesses. The importance of organizing meetings from daily stand ups or scrums to knowledge transfer or knowledge sharing sessions and webinars is invaluable, thus helping various team members to stay informed and gain clarity of the tasks for informed decision making and boosting productivity.\n",
    " \n",
    "It is often seen for individuals who fail to catch up with such critical meetings had to face the challenge of going through long meeting recordings along with their ongoing work leading to bandwidth inefficiency as this process is time consuming and inefficient. On the other hand, creating standard operational procedures or documentations manually from KT sessions or webinars is again labor intensive and creates a productivity bottleneck.\n",
    " \n",
    "With that in mind, we have implemented an automated and easy to integrate plug-and-play framework leverages state-of-the-art transcription and diarization techniques to transform lengthy audio recordings into concise, actionable digests, significantly enhancing productivity and collaboration and increasing operational efficiency.\n",
    " \n",
    "Coming to the objectives and methodology, we have employed a very simple, yet sophisticated approach, which comprises of 4 major pipeline components - Audio Transcription and Diarization, LLM Powered Content, Insight and Action Generation, Audio Digest generation from LLM Response, and finally an MLOps driven plug-and-play framework.\n",
    " \n",
    "For the first layer, we have our transcriptor which generates transcriptions from the meeting audio, and a diarizer which assigns unique speaker ID to the transcribed information. We have implemented various transcription and diarization models, libraries - both open source and enterprise cloud services and thoroughly observed the quality of the diarized transcripts generated. Upon thorough experimentation, we concluded that Azure's Real Time Diarizer is seen to outperform other open source libraries and models like PyAnnote and OpenAI's Whisper, which is an Audio Transformer Model developed for ASR or Automatic Speech Recognition. Azure RTD was seen to outperform not only in terms of speed, but also in terms of efficiency and scalability and accuracy.\n",
    " \n",
    "On having the inputs to our LLM ready - which is the meeting transcript, we move ahead to the LLM generation and prompt engineering layer. Here we are sending our call transcripts, as context to a Large Language Model for generating context specific outputs. What we mean by context specific outputs is, we are handling variety of meeting recordings, as discussed earlier - it can be a daily standup call, or a project discussion and also any Knowledge Transfer session or webinar. For the same reason, the outputs need to be tailored to the type of the meeting. it assigns tasks (actions) to team members for project/sprint meetings whereas creates documentation and SOPs from webinars or KTs.- We were able to achieve this one-to-many use-case mapping by leveraging advanced prompt engineering and few-shot prompting techniques, where we direct the LLM to analyze the transcript and produce context specific output.\n",
    " \n",
    "Next, we have added a Text2Speech feature which generates a short and concise audio digest from the LLM output generated. We have added this feature keeping in mind the user preferences to often go for audio digests rather than reading the AI generated output. For this we leveraged Azure Speech Services for speech generation from LLMâ€™s output ensuring enhanced audio quality supporting up to 12 different accents.\n",
    " \n",
    "Coming to the final layer, we have refactored the entire pipeline into standalone services and routes using advance MLOps practices like a feature store for model parameter tracking, performance evaluation, dashboards for app usage and cost metrics, wrapped in a one go cloud deployment and app serving. Due to this highly flexible and integration-easy workflow, our framework offers a plug-and-play architecture for developers, enabling seamless integration/replacement of various pipeline components.\n",
    " \n",
    "Coming to the solution architecture and workflow, as you can see, we have a blob trigger service, which creates an automated trigger response as soon as a new recorded meeting gets uploaded to the org's SharePoint. Next, we take the meeting file and do several audio preprocessing steps like video to audio conversion, followed by audio's stereo to mono channel conversion and finally audio amplification to enhance overall audio quality. Once the audio is processed, we upload it to our cloud storage service. From cloud storage service the audio file is routed to the Azure Real Time Diarizer endpoint where real-time transcription and diarization process begins. Once the transcript is ready we store it in our cloud storage and further route it to the custom LLM endpoint in Azure ML Studio. We have opted for Azure ML Studio to leverage GPU computes for faster LLM responses. Once we have the output ready from the LLM, we store it again to our same storage bucket and send the output to the end user using a FAST API endpoint which supports asynchronous requests for efficient load balancing on multiple meetings uploaded at the same time to the pipeline.\n",
    " \n",
    "Coming to the dataset and evaluation part, we have leveraged our real-world proprietary dataset that comprises of scrum call recordings, knowledge sharing sessions, project discussions and webinars. All models, libraries, and services were assessed using original transcriptions, with performance measured by Word Error Rate (WER) and BERTScore, while generated audio was rated on a 0-5 scale using Mean Opinion Score by our SMEs. We achieved the highest scores for general meeting discussions which were recordings of brainstorming sessions, solving bugs or issues in a call, team alignment discussions or product review calls with customers. We also managed to achieve decent scores for other recording data comprising of KTs, Webinars, product demos and team scrums.\n",
    " \n",
    "With that I would like to highlight some key takeaways on how our approach and end to end solution addresses productivity bottlenecks, being highly flexible and customizable to one's preferred tools and needs, and being integration-easy due to robust MLOps driven development and deployment.\n",
    " \n",
    "Planning for the future, we have targeted to integrate our solution to popular project management tools and workflows to automate and manage task assignments in complex and long term projects to streamline workflow further and provide a more cohesive and collaborative environment.\n",
    " \n",
    "With that I would like to end my presentation. I can take questions from here on. Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
